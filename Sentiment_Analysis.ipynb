{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, plot_confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "import contractions\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_reviews.csv', converters={'cleaned_text': literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sentiment.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = ['cannot', 'not', 'nor', 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in stop_list:\n",
    "    # Add the word to the set of stop words. Use lowercase!\n",
    "    nlp.Defaults.stop_words.remove(item)\n",
    "    \n",
    "    # Set the stop_word tag on the lexeme\n",
    "    nlp.vocab[item].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I got these headphones as a xmas for my wife (yeah for me too). I own multiple pairs of corded headphones and one other pair of cheap bluetooth headphones (Kinivo BTH240). My daily go to headphones are a pair of Sennheiser hd 280 pro cans and I have a pair of Audio-Technica ATH-M30 that I use at work.\\n\\nRatings:\\n\\nSound: 8/10\\n\\nThe sound is fairly detailed and well balanced. The bass end is not overpowering nor underwhelming. Midrange is clear and distinct. High end has detail and it is not too tinny. YMMV. I would say, for the headphones I use, the sound reproduction is somewhere between my Audio-Technica ATH-M30 but not quite as good as my Sennheiser hd 280. Which is on par for the price range they are in. Come on people, stop comparing these to headphones that cost 3-5 times as much.\\n\\nComfort: 10/10\\n\\nThese fit over my ears well and these may very well be the most comfortable headphones I presently own. Most headphones I wear do not bother me much and these are no exception.\\n\\nBuild quality: 10/10\\n\\nThese headphones have a solid feel to them; a lot like a high end Benz car door. Only time will tell how durable they will be but they give me a lot of confidence.\\n\\nNoise cancellation: 8/10\\n\\nI have read other reviews and I am astounded people would be audacious enough to compare these headphones to Bose QC25 or 35s. This is frankly ridiculous. I started with a 7 then moved it to 7.5 but really forgive me in giving this 8 out of 10. For the price range, the sound quality with NC is well above average; just below outstanding. Any active noise cancellation will color the sound. This is just physics. These headphones do a great job of delivering great sound while reducing the ambient low end noise without coloring the music too much. To experiment, take these headphones into your car and play no music in them but turn on the car with the fan on. Turn the noise cancellation on and off and on again. The low end frequencies will be absent while higher frequencies still remain. I use Bose A20 aviation headsets which have far superior noise reduction but the noise cancellation performance above 500 hz would only be marginally different. I would assume the QC25 would be the same and I wouldn't spend another $200 for that minute performance difference.\\n\\nWireless performance: 9/10\\n\\nI have been really impressed by the bluetooth performance. I own a pair of $25 bluetooth headphones. I can't leave the room without dropping. The range is much better. Of course bluetooth is really meant for LOS (line of sight) and it would be asking a lot of any bluetooth device to successfully transmit thru a wall in another room. But these did fine in nLOS (near line of sight) with a distance of 25 ft. Also these did very well on a phone call. The other person couldn't even tell I was on a bluetooth headset.\\n\\nPerceived Value: 9/10\\n\\nFor one Ben Franklin, I am well please with the value delivered thus far. I am reasonably confident these headphones will last for some time. If you are looking for an all purpose bluetooth headphones, you can't go wrong with these headphones. I highly recommend them. Love these cordless cans!\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reviews[132512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'headphone',\n",
       " 'xma',\n",
       " 'wife',\n",
       " 'multiple',\n",
       " 'pair',\n",
       " 'cord',\n",
       " 'headphone',\n",
       " 'pair',\n",
       " 'cheap',\n",
       " 'bluetooth',\n",
       " 'headphone',\n",
       " 'daily',\n",
       " 'headphone',\n",
       " 'pair',\n",
       " 'can',\n",
       " 'pair',\n",
       " 'use',\n",
       " 'work',\n",
       " 'sound',\n",
       " 'sound',\n",
       " 'fairly',\n",
       " 'detailed',\n",
       " 'balance',\n",
       " 'bass',\n",
       " 'end',\n",
       " 'overpower',\n",
       " 'underwhelme',\n",
       " 'clear',\n",
       " 'distinct',\n",
       " 'high',\n",
       " 'end',\n",
       " 'detail',\n",
       " 'tinny',\n",
       " 'headphone',\n",
       " 'use',\n",
       " 'sound',\n",
       " 'reproduction',\n",
       " 'good',\n",
       " 'par',\n",
       " 'price',\n",
       " 'range',\n",
       " 'people',\n",
       " 'stop',\n",
       " 'compare',\n",
       " 'headphone',\n",
       " 'cost',\n",
       " 'time',\n",
       " 'fit',\n",
       " 'ear',\n",
       " 'comfortable',\n",
       " 'headphone',\n",
       " 'presently',\n",
       " 'headphone',\n",
       " 'wear',\n",
       " 'bother',\n",
       " 'exception',\n",
       " 'build',\n",
       " 'quality',\n",
       " 'headphone',\n",
       " 'solid',\n",
       " 'feel',\n",
       " 'lot',\n",
       " 'high',\n",
       " 'end',\n",
       " 'car',\n",
       " 'door',\n",
       " 'time',\n",
       " 'tell',\n",
       " 'durable',\n",
       " 'lot',\n",
       " 'confidence',\n",
       " 'cancellation',\n",
       " 'read',\n",
       " 'review',\n",
       " 'astounded',\n",
       " 'people',\n",
       " 'audacious',\n",
       " 'compare',\n",
       " 'headphone',\n",
       " 'frankly',\n",
       " 'ridiculous',\n",
       " 'start',\n",
       " 'move',\n",
       " 'forgive',\n",
       " 'give',\n",
       " 'price',\n",
       " 'range',\n",
       " 'sound',\n",
       " 'quality',\n",
       " 'average',\n",
       " 'outstanding',\n",
       " 'active',\n",
       " 'noise',\n",
       " 'cancellation',\n",
       " 'color',\n",
       " 'sound',\n",
       " 'physics',\n",
       " 'headphone',\n",
       " 'great',\n",
       " 'job',\n",
       " 'deliver',\n",
       " 'great',\n",
       " 'sound',\n",
       " 'reduce',\n",
       " 'ambient',\n",
       " 'low',\n",
       " 'end',\n",
       " 'noise',\n",
       " 'color',\n",
       " 'music',\n",
       " 'experiment',\n",
       " 'headphone',\n",
       " 'car',\n",
       " 'play',\n",
       " 'music',\n",
       " 'turn',\n",
       " 'car',\n",
       " 'fan',\n",
       " 'turn',\n",
       " 'noise',\n",
       " 'cancellation',\n",
       " 'low',\n",
       " 'end',\n",
       " 'frequency',\n",
       " 'absent',\n",
       " 'high',\n",
       " 'frequency',\n",
       " 'remain',\n",
       " 'use',\n",
       " 'aviation',\n",
       " 'headset',\n",
       " 'far',\n",
       " 'superior',\n",
       " 'noise',\n",
       " 'reduction',\n",
       " 'noise',\n",
       " 'cancellation',\n",
       " 'performance',\n",
       " 'marginally',\n",
       " 'different',\n",
       " 'assume',\n",
       " 'spend',\n",
       " 'minute',\n",
       " 'performance',\n",
       " 'difference',\n",
       " 'wireless',\n",
       " 'performance',\n",
       " 'impress',\n",
       " 'bluetooth',\n",
       " 'performance',\n",
       " 'pair',\n",
       " 'bluetooth',\n",
       " 'headphone',\n",
       " 'leave',\n",
       " 'room',\n",
       " 'drop',\n",
       " 'range',\n",
       " 'well',\n",
       " 'course',\n",
       " 'bluetooth',\n",
       " 'mean',\n",
       " 'line',\n",
       " 'sight',\n",
       " 'ask',\n",
       " 'lot',\n",
       " 'bluetooth',\n",
       " 'device',\n",
       " 'successfully',\n",
       " 'transmit',\n",
       " 'wall',\n",
       " 'room',\n",
       " 'fine',\n",
       " 'line',\n",
       " 'sight',\n",
       " 'distance',\n",
       " 'phone',\n",
       " 'person',\n",
       " 'tell',\n",
       " 'bluetooth',\n",
       " 'headset',\n",
       " 'perceive',\n",
       " 'value',\n",
       " 'deliver',\n",
       " 'far',\n",
       " 'reasonably',\n",
       " 'confident',\n",
       " 'headphone',\n",
       " 'time',\n",
       " 'look',\n",
       " 'purpose',\n",
       " 'bluetooth',\n",
       " 'headphone',\n",
       " 'wrong',\n",
       " 'headphone',\n",
       " 'highly',\n",
       " 'recommend',\n",
       " 'love',\n",
       " 'cordless',\n",
       " 'can']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cleaned_text[132512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(data.reviews[132512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Replace &nbsp; with regular space\n",
    "    text = text.replace(\"&nbsp;\", \" \")\n",
    "    # Remove HTML tags and attributes\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove line breaks and extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Remove numbers and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Convert to lowercase and remove leading/trailing white space\n",
    "    # Tokenize and lemmatize\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_.lower().strip() for token in doc if not token.is_punct and not token.is_stop and len(token) > 1]\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text_sent'] = data.reviews.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/cleaned_reviews.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>cleaned_text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Get the SportaPros instead.  They look better,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[instead, look, well, wear, street, configurat...</td>\n",
       "      <td>4</td>\n",
       "      <td>[sportapros, instead, look, well, wear, street...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've been looking for a lighter alternative to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[look, light, alternative, absolutely, perfect...</td>\n",
       "      <td>2</td>\n",
       "      <td>[look, light, alternative, absolutely, perfect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The finest headphones available. You can spend...</td>\n",
       "      <td>1</td>\n",
       "      <td>[fine, headphone, available, spend, vast, amou...</td>\n",
       "      <td>2</td>\n",
       "      <td>[fine, headphone, available, spend, vast, amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rd pair of these I've purchased.  My wife has...</td>\n",
       "      <td>1</td>\n",
       "      <td>[pair, purchase, wife, pair, pair, glove, box,...</td>\n",
       "      <td>3</td>\n",
       "      <td>[rd, pair, purchase, wife, pair, pair, glove, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My old Koss Porta Pros finally got beat to dea...</td>\n",
       "      <td>1</td>\n",
       "      <td>[old, finally, get, beat, death, year, ago, la...</td>\n",
       "      <td>4</td>\n",
       "      <td>[old, koss, porta, pros, finally, get, beat, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment  \\\n",
       "0  Get the SportaPros instead.  They look better,...          1   \n",
       "1  I've been looking for a lighter alternative to...          1   \n",
       "2  The finest headphones available. You can spend...          1   \n",
       "3  3rd pair of these I've purchased.  My wife has...          1   \n",
       "4  My old Koss Porta Pros finally got beat to dea...          1   \n",
       "\n",
       "                                        cleaned_text  dominant_topic  \\\n",
       "0  [instead, look, well, wear, street, configurat...               4   \n",
       "1  [look, light, alternative, absolutely, perfect...               2   \n",
       "2  [fine, headphone, available, spend, vast, amou...               2   \n",
       "3  [pair, purchase, wife, pair, pair, glove, box,...               3   \n",
       "4  [old, finally, get, beat, death, year, ago, la...               4   \n",
       "\n",
       "                                   cleaned_text_sent  \n",
       "0  [sportapros, instead, look, well, wear, street...  \n",
       "1  [look, light, alternative, absolutely, perfect...  \n",
       "2  [fine, headphone, available, spend, vast, amou...  \n",
       "3  [rd, pair, purchase, wife, pair, pair, glove, ...  \n",
       "4  [old, koss, porta, pros, finally, get, beat, d...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range = (1,1), \n",
    "                     stop_words = None, \n",
    "                     tokenizer = dummy, \n",
    "                     preprocessor = dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(text, n, vect):\n",
    "    # Function using count vectorizer to return top n frequent words\n",
    "    words = vect.fit_transform(text)\n",
    "    sum_words = words.sum(axis=0)\n",
    "    word_freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n",
    "    word_df = pd.DataFrame(word_freq, columns=['word', 'count'])\n",
    "    return word_df[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos = top_words(text = data[data.sentiment == 1].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = vect)\n",
    "top30_neg = top_words(text = data[data.sentiment == 0].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos = top_words(text = data[(data.sentiment == 1)&(data.dominant_topic == 2)].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv)\n",
    "top30_neg = top_words(text = data[(data.sentiment == 0)&(data.dominant_topic == 2)].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos3 = data[(data.sentiment == 1)&(data.dominant_topic == 2)].cleaned_text\n",
    "neg3 = data[(data.sentiment == 0)&(data.dominant_topic == 2)].cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,10))\n",
    "sns.barplot(data = top30_pos, x = 'count', y = 'word', orient = 'h', ax = axs[0])\n",
    "sns.barplot(data = top30_neg, x = 'count', y = 'word', orient = 'h', ax = axs[1])\n",
    "axs[0].set_title('Top 30 Frequent Positive Words')\n",
    "axs[1].set_title('Top 30 Frequent Negative Words')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_bigram = CountVectorizer(ngram_range = (2,2), \n",
    "                     stop_words = None, \n",
    "                     tokenizer = dummy, \n",
    "                     preprocessor = dummy,\n",
    "                           max_df = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos_bigrams = top_words(text = data[data.sentiment == 1].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv_bigram)\n",
    "top30_neg_bigrams = top_words(text = data[data.sentiment == 0].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos_bigrams = top_words(text = pos3, \n",
    "                              n = 30, \n",
    "                              vect = cv_bigram)\n",
    "top30_neg_bigrams = top_words(text = neg3, \n",
    "                              n = 30, \n",
    "                              vect = cv_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,10))\n",
    "sns.barplot(data = top30_pos_bigrams, x = 'count', y = 'word', orient = 'h', ax = axs[0])\n",
    "sns.barplot(data = top30_neg_bigrams, x = 'count', y = 'word', orient = 'h', ax = axs[1])\n",
    "axs[0].set_title('Top 30 Frequent Positive Bigrams')\n",
    "axs[1].set_title('Top 30 Frequent Negative Bigrams')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_trigram = CountVectorizer(ngram_range = (3,3), \n",
    "                     stop_words = None, \n",
    "                     tokenizer = dummy, \n",
    "                     preprocessor = dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos_trigrams = top_words(text = data[data.sentiment == 1].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv_trigram)\n",
    "top30_neg_trigrams = top_words(text = data[data.sentiment == 0].cleaned_text, \n",
    "                              n = 30, \n",
    "                              vect = cv_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_pos_trigrams = top_words(text = pos3, \n",
    "                              n = 30, \n",
    "                              vect = cv_trigram)\n",
    "top30_neg_trigrams = top_words(text = neg3, \n",
    "                              n = 30, \n",
    "                              vect = cv_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,10))\n",
    "sns.barplot(data = top30_pos_trigrams, x = 'count', y = 'word', orient = 'h', ax = axs[0])\n",
    "sns.barplot(data = top30_neg_trigrams, x = 'count', y = 'word', orient = 'h', ax = axs[1])\n",
    "axs[0].set_title('Top 30 Frequent Positive Trigrams')\n",
    "axs[1].set_title('Top 30 Frequent Negative Trigrams')\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.cleaned_text\n",
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range = (1,1), \n",
    "                             stop_words = None, \n",
    "                             tokenizer = dummy, \n",
    "                             preprocessor = dummy, \n",
    "                             min_df = 10,\n",
    "                             max_df = 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range = (1,3), \n",
    "                             stop_words = None, \n",
    "                             tokenizer = dummy, \n",
    "                             preprocessor = dummy, \n",
    "                             min_df = 10,\n",
    "                             max_df = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr = LogisticRegression(class_weight = 'balanced', solver = 'saga', max_iter = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_cv_model = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('logreg', logr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_logr = cross_validate(logr_cv_model, X_train, y_train, cv = 5, scoring = ['f1', 'precision', 'recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame(columns=['Model', 'mean f1_score', 'std f1_score', 'mean precision', 'mean recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'logred_cv_model',\n",
    "                      'mean f1_score': round(cv_logr['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(cv_logr['test_f1'].std(), 4),\n",
    "                      'mean precision': round(cv_logr['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(cv_logr['test_recall'].mean(), 4)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(class_prior = [0.23, 0.77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_cv_model = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('mnb', mnb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_mnb = cross_validate(mnb_cv_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'mnb_cv_model',\n",
    "                      'mean f1_score': round(cv_mnb['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(cv_mnb['test_f1'].std(), 4),\n",
    "                      'mean precision': round(cv_mnb['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(cv_mnb['test_recall'].mean(), 4)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_cv_model = Pipeline([\n",
    "    ('vectorizer', count_vect),\n",
    "    ('rfc', rfc)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rf = cross_validate(rfc_cv_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'rfc_cv_model',\n",
    "                      'mean f1_score': round(cv_rf['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(cv_rf['test_f1'].std(), 4),\n",
    "                      'mean precision': round(cv_rf['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(cv_rf['test_recall'].mean(), 4)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1,1), \n",
    "                             stop_words = None, \n",
    "                             tokenizer = dummy, \n",
    "                             preprocessor = dummy, \n",
    "                             min_df = 10,\n",
    "                             max_df = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_tfidf_model = Pipeline([\n",
    "    ('vectorizer', tfidf),\n",
    "    ('logreg', logr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logr = cross_validate(logr_tfidf_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'tfidf_logr_model',\n",
    "                      'mean f1_score': round(tfidf_logr['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(tfidf_logr['test_f1'].std(), 4),\n",
    "                      'mean precision': round(tfidf_logr['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(tfidf_logr['test_recall'].mean(), 4)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_tfidf_model = Pipeline([\n",
    "    ('vectorizer', tfidf),\n",
    "    ('clf', mnb)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mnb = cross_validate(mnb_tfidf_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'tfidf_mnb_model',\n",
    "                      'mean f1_score': round(tfidf_mnb['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(tfidf_mnb['test_f1'].std(), 4),\n",
    "                      'mean precision': round(tfidf_mnb['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(tfidf_mnb['test_recall'].mean(), 4)}, ignore_index = True)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tfidf_model = Pipeline([\n",
    "    ('vectorizer', tfidf),\n",
    "    ('clf', rfc)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_rf = cross_validate(rfc_tfidf_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'tfidf_rf_model',\n",
    "                      'mean f1_score': round(tfidf_rf['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(tfidf_rf['test_f1'].std(), 4),\n",
    "                      'mean precision': round(tfidf_rf['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(tfidf_rf['test_recall'].mean(), 4)}, ignore_index = True)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary = set(word for review in reviews for word in review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('data/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove['great']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_w2v_model = Pipeline([\n",
    "    ('vectorizer', W2vVectorizer(glove)),\n",
    "    ('logreg', logr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_logr = cross_validate(logr_tfidf_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'w2v_logr_model',\n",
    "                      'mean f1_score': round(w2v_logr['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(w2v_logr['test_f1'].std(), 4),\n",
    "                      'mean precision': round(w2v_logr['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(w2v_logr['test_recall'].mean(), 4)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = model_results.append({'Model': 'mnb_w2v_model',\n",
    "                      'mean f1_score': round(w2v_mnb['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(w2v_mnb['test_f1'].std(), 4),\n",
    "                      'mean precision': round(w2v_mnb['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(w2v_mnb['test_recall'].mean(), 4)}, ignore_index = True)\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rfc_w2v_model = Pipeline([\n",
    "    ('vectorizer',  W2vVectorizer(glove)),\n",
    "    ('rfc', rfc)\n",
    "])\n",
    "\n",
    "w2v_rf = cross_validate(rfc_w2v_model, X_train, y_train, cv = 5, scoring = ['precision', 'recall', 'f1'])\n",
    "\n",
    "\n",
    "\n",
    "model_results = model_results.append({'Model': 'rfc_w2v_model',\n",
    "                      'mean f1_score': round(w2v_rf['test_f1'].mean(), 4),\n",
    "                      'std f1_score': round(w2v_rf['test_f1'].std(), 4),\n",
    "                      'mean precision': round(w2v_rf['test_precision'].mean(), 4),\n",
    "                      'mean recall': round(w2v_rf['test_recall'].mean(), 4)}, ignore_index = True)\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
